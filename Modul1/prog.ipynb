{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fd30d74",
   "metadata": {},
   "source": [
    "Question 1 â€“ S&P 500 Stocks Added to the Index\n",
    "Goal:\n",
    "Scrape S&P 500 company data from Wikipedia.\n",
    "\n",
    "Extract the year each company was added.\n",
    "\n",
    "Count how many were added each year.\n",
    "\n",
    "Exclude 1957.\n",
    "\n",
    "Find the year with the most additions (most recent if tied).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e92b540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Year with the most additions to S&P 500: 2017\n",
      "ðŸ“Š Number of companies added that year: 23\n",
      "ðŸ“ˆ Companies in the index for more than 20 years: 166\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from Wikipedia\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "# First table contains current S&P 500 companies\n",
    "df = tables[0]\n",
    "\n",
    "# Check if 'Date added' exists and is not null\n",
    "if 'Date added' in df.columns:\n",
    "    df = df[df['Date added'].notna()]\n",
    "    df['Year added'] = pd.to_datetime(df['Date added']).dt.year\n",
    "    df = df[df['Year added'] != 1957]  # Exclude 1957\n",
    "\n",
    "    # Count how many companies were added each year\n",
    "    additions_by_year = df['Year added'].value_counts().sort_index()\n",
    "\n",
    "    # Find the year with the most additions (latest if tie)\n",
    "    max_additions = additions_by_year.max()\n",
    "    best_year = additions_by_year[additions_by_year == max_additions].index.max()\n",
    "\n",
    "    print(\"âœ… Year with the most additions to S&P 500:\", best_year)\n",
    "    print(\"ðŸ“Š Number of companies added that year:\", max_additions)\n",
    "\n",
    "    # Bonus: how many have been in the index > 20 years?\n",
    "    current_year = pd.Timestamp.now().year\n",
    "    df['Years in index'] = current_year - df['Year added']\n",
    "    over_20_years = df[df['Years in index'] > 20].shape[0]\n",
    "    print(\"ðŸ“ˆ Companies in the index for more than 20 years:\", over_20_years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc150729",
   "metadata": {},
   "source": [
    "Question 2 â€“ YTD Index Returns vs. S&P 500\n",
    "Goal:\n",
    "Compare YTD performance of major world indexes vs. the S&P 500 from Jan 1 to May 1, 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a53ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get ticker '^GSPC' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['^GSPC']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "Failed to get ticker '000001.SS' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['000001.SS']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "Failed to get ticker '^HSI' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['^HSI']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "Failed to get ticker '^AXJO' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['^AXJO']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "Failed to get ticker '^NSEI' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['^NSEI']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "Failed to get ticker '^GSPTSE' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['^GSPTSE']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "Failed to get ticker '^GDAXI' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['^GDAXI']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "Failed to get ticker '^FTSE' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['^FTSE']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "Failed to get ticker '^N225' reason: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Not enough data for US (^GSPC)\n",
      "âš ï¸ Not enough data for China (000001.SS)\n",
      "âš ï¸ Not enough data for Hong Kong (^HSI)\n",
      "âš ï¸ Not enough data for Australia (^AXJO)\n",
      "âš ï¸ Not enough data for India (^NSEI)\n",
      "âš ï¸ Not enough data for Canada (^GSPTSE)\n",
      "âš ï¸ Not enough data for Germany (^GDAXI)\n",
      "âš ï¸ Not enough data for UK (^FTSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['^N225']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "Failed to get ticker '^MXX' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['^MXX']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "Failed to get ticker '^BVSP' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['^BVSP']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Not enough data for Japan (^N225)\n",
      "âš ï¸ Not enough data for Mexico (^MXX)\n",
      "âš ï¸ Not enough data for Brazil (^BVSP)\n",
      "âœ… Number of indexes with better YTD returns than S&P 500: 0\n",
      "\n",
      "ðŸ“Š YTD Returns (Janâ€“May 2025):\n",
      "Series([], dtype: object)\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "indices = {\n",
    "    'US': '^GSPC',\n",
    "    'China': '000001.SS',\n",
    "    'Hong Kong': '^HSI',\n",
    "    'Australia': '^AXJO',\n",
    "    'India': '^NSEI',\n",
    "    'Canada': '^GSPTSE',\n",
    "    'Germany': '^GDAXI',\n",
    "    'UK': '^FTSE',\n",
    "    'Japan': '^N225',\n",
    "    'Mexico': '^MXX',\n",
    "    'Brazil': '^BVSP'\n",
    "}\n",
    "\n",
    "start_date = '2025-01-01'\n",
    "end_date = '2025-05-01'\n",
    "returns = {}\n",
    "\n",
    "for country, ticker in indices.items():\n",
    "    try:\n",
    "        data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "\n",
    "        if len(data) >= 2:\n",
    "            first_close = data['Close'].iloc[0]\n",
    "            last_close = data['Close'].iloc[-1]\n",
    "            ytd_return = (last_close / first_close - 1) * 100\n",
    "            returns[country] = ytd_return\n",
    "        else:\n",
    "            print(f\"âš ï¸ Not enough data for {country} ({ticker})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching data for {country} ({ticker}): {e}\")\n",
    "\n",
    "# Convert to Series and compare\n",
    "returns_df = pd.Series(returns).sort_values(ascending=False)\n",
    "snp_return = returns.get('US', 0)\n",
    "better_than_snp = returns_df[returns_df > snp_return].count()\n",
    "\n",
    "print(\"âœ… Number of indexes with better YTD returns than S&P 500:\", better_than_snp)\n",
    "print(\"\\nðŸ“Š YTD Returns (Janâ€“May 2025):\")\n",
    "print(returns_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0badd1b",
   "metadata": {},
   "source": [
    "Question 3 â€“ S&P 500 Correction Analysis\n",
    "Goal:\n",
    "Define corrections as >5% drawdown from previous all-time high.\n",
    "\n",
    "Calculate duration of each correction.\n",
    "\n",
    "Report median, 25th, and 75th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa0b8760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get ticker '^GSPC' reason: Expecting value: line 1 column 1 (char 0)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['^GSPC']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Median correction duration (days): nan\n",
      "ðŸ“Š Percentiles:\n",
      "25th percentile: nan\n",
      "75th percentile: nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Download long-term S&P 500 data\n",
    "snp = yf.download('^GSPC', start='1950-01-01')\n",
    "\n",
    "# Identify all-time highs and drawdowns\n",
    "snp['cummax'] = snp['Close'].cummax()\n",
    "snp['drawdown'] = (snp['Close'] - snp['cummax']) / snp['cummax']\n",
    "\n",
    "# Mark start and end of corrections (drawdown > 5%)\n",
    "snp['correction'] = snp['drawdown'] < -0.05\n",
    "corrections = []\n",
    "in_correction = False\n",
    "\n",
    "for i in range(1, len(snp)):\n",
    "    if snp['correction'].iloc[i] and not in_correction:\n",
    "        start = snp.index[i]\n",
    "        in_correction = True\n",
    "    elif not snp['correction'].iloc[i] and in_correction:\n",
    "        end = snp.index[i]\n",
    "        duration = (end - start).days\n",
    "        corrections.append(duration)\n",
    "        in_correction = False\n",
    "\n",
    "# Analyze durations\n",
    "corrections_series = pd.Series(corrections)\n",
    "print(\"âœ… Median correction duration (days):\", corrections_series.median())\n",
    "print(\"ðŸ“Š Percentiles:\")\n",
    "print(\"25th percentile:\", corrections_series.quantile(0.25))\n",
    "print(\"75th percentile:\", corrections_series.quantile(0.75))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85117ba8",
   "metadata": {},
   "source": [
    "Question 4 â€“ Amazon Earnings Surprise Analysis\n",
    "Make sure you have the file ha1_Amazon.csv in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2aac8d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ha1_Amazon.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load earnings data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_eps \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mha1_Amazon.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df_eps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_eps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Download historical price data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ha1_Amazon.csv'"
     ]
    }
   ],
   "source": [
    "# Load earnings data\n",
    "df_eps = pd.read_csv(\"ha1_Amazon.csv\", delimiter=';')\n",
    "df_eps['Date'] = pd.to_datetime(df_eps['Date'])\n",
    "\n",
    "# Download historical price data\n",
    "amzn = yf.download(\"AMZN\", start=\"2010-01-01\")\n",
    "\n",
    "# Compute 2-day return: Day3 / Day1 - 1\n",
    "amzn['2d_return'] = amzn['Close'].shift(-2) / amzn['Close'] - 1\n",
    "\n",
    "# Filter for positive earnings surprises\n",
    "df_eps = df_eps[df_eps['Actual EPS'] > df_eps['Estimate EPS']]\n",
    "results = []\n",
    "\n",
    "# Find 2-day return following each earnings surprise\n",
    "for date in df_eps['Date']:\n",
    "    if date in amzn.index:\n",
    "        idx = amzn.index.get_loc(date)\n",
    "        if idx + 2 < len(amzn):\n",
    "            ret = amzn.iloc[idx]['2d_return']\n",
    "            results.append(ret)\n",
    "\n",
    "# Median 2-day return after surprise\n",
    "results_series = pd.Series(results)\n",
    "median_surprise_return = results_series.median() * 100\n",
    "\n",
    "print(\"âœ… Median 2-day return after positive earnings surprise:\", round(median_surprise_return, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2e65d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
